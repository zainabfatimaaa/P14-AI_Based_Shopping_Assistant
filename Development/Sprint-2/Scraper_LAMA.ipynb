{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcbb4898",
   "metadata": {},
   "source": [
    "# LAMA Scraping Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b01c2dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import asyncio\n",
    "import time\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import os\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "from bson import Binary\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import nest_asyncio\n",
    "import re\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35642750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total links fetched: 1793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating links: 100%|█████████████████████| 1793/1793 [02:16<00:00, 13.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid links: 1787\n",
      "Invalid links: 6\n",
      "Removed 6 invalid links from MongoDB.\n",
      "Total valid links: 1787\n"
     ]
    }
   ],
   "source": [
    "# MongoDB Configuration\n",
    "MONGODB_URI = \"mongodb+srv://AhmadJabbar:0uU29STyRwhoxV0X@shopsavvy.xaqy1.mongodb.net/\"\n",
    "DATABASE_NAME = \"test\"\n",
    "COLLECTION_NAME = \"products\"\n",
    "\n",
    "# Create a folder for saving images if it doesn't exist\n",
    "os.makedirs(\"product_images\", exist_ok=True)\n",
    "\n",
    "# MongoDB Utility Functions\n",
    "def get_mongo_client():\n",
    "    return MongoClient(MONGODB_URI)\n",
    "\n",
    "def fetch_all_links():\n",
    "    \"\"\"Fetch all product links from MongoDB.\"\"\"\n",
    "    try:\n",
    "        client = MongoClient(MONGODB_URI)\n",
    "        db = client[DATABASE_NAME]\n",
    "        collection = db[COLLECTION_NAME]\n",
    "        \n",
    "        # fetch only the 'link' field for all products\n",
    "        links = [product['link'] for product in collection.find() if 'link' in product]\n",
    "        print(f\"Total links fetched: {len(links)}\")\n",
    "        return links\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching links: {e}\")\n",
    "        return []\n",
    "    finally:\n",
    "        client.close()\n",
    "\n",
    "def validate_link(link):\n",
    "    \"\"\"Check if the link is valid.\"\"\"\n",
    "    try:\n",
    "        response = requests.head(link, timeout=10)  # Use HEAD request for faster validation\n",
    "        if response.status_code == 200:\n",
    "            return (link, True)  # Link is valid\n",
    "        else:\n",
    "            return (link, False)  # Link is invalid\n",
    "    except requests.RequestException as e:\n",
    "        return (link, False)  # Link is invalid or unreachable\n",
    "\n",
    "def validate_all_links(links):\n",
    "    \"\"\"Validate all links concurrently with progress bar.\"\"\"\n",
    "    valid_links = []\n",
    "    invalid_links = []\n",
    "    \n",
    "    # create a progress bar using tqdm\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor: \n",
    "        futures = {executor.submit(validate_link, link): link for link in links}\n",
    "        \n",
    "        # Use tqdm to show progress for the number of completed tasks\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Validating links\"):\n",
    "            link = futures[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                if result[1]:\n",
    "                    valid_links.append(result[0])\n",
    "                else:\n",
    "                    invalid_links.append(result[0])\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {link}: {e}\")\n",
    "    \n",
    "    print(f\"Valid links: {len(valid_links)}\")\n",
    "    print(f\"Invalid links: {len(invalid_links)}\")\n",
    "    return valid_links, invalid_links\n",
    "\n",
    "def remove_invalid_links_from_mongodb(invalid_links):\n",
    "    \"\"\"Remove invalid links from MongoDB.\"\"\"\n",
    "    try:\n",
    "        client = get_mongo_client()\n",
    "        db = client[DATABASE_NAME]\n",
    "        collection = db[COLLECTION_NAME]\n",
    "        \n",
    "        # Remove the invalid links from the database\n",
    "        collection.delete_many({\"link\": {\"$in\": invalid_links}})\n",
    "        print(f\"Removed {len(invalid_links)} invalid links from MongoDB.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error removing invalid links from MongoDB: {e}\")\n",
    "    finally:\n",
    "        client.close()\n",
    "\n",
    "# Main workflow\n",
    "def main():\n",
    "    # Fetch all links from MongoDB\n",
    "    links = fetch_all_links()\n",
    "    \n",
    "    # Validate all links\n",
    "    valid_links, invalid_links = validate_all_links(links)\n",
    "    \n",
    "    # Remove invalid links from MongoDB\n",
    "    remove_invalid_links_from_mongodb(invalid_links)\n",
    "    \n",
    "    # Optionally, return valid links for further processing\n",
    "    return valid_links\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    valid_links = main()\n",
    "    print(f\"Total valid links: {len(valid_links)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b25a2ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrolling to load all products...\n",
      "Scrolling to load all products...\n",
      "Scrolling to load all products...\n",
      "Scrolling to load all products...\n",
      "Scrolling to load all products...\n",
      "Found 17 product links.\n",
      "Scraping 0 new product links.\n",
      "Found 19 product links.\n",
      "Scraping 0 new product links.\n",
      "Found 54 product links.\n",
      "Scraping 0 new product links.\n",
      "Found 73 product links.\n",
      "Scraping 0 new product links.\n",
      "Found 107 product links.\n",
      "Scraping 0 new product links.\n",
      "0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "async def scrape_data(url, product_type, gender):\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(url)\n",
    "\n",
    "        # Scroll to load all products\n",
    "        print(\"Scrolling to load all products...\")\n",
    "        previous_height = await page.evaluate(\"document.body.scrollHeight\")\n",
    "        \n",
    "        while True:\n",
    "            await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "            await page.wait_for_timeout(5000)\n",
    "            \n",
    "            new_height = await page.evaluate(\"document.body.scrollHeight\")\n",
    "            if new_height == previous_height:\n",
    "                break\n",
    "            previous_height = new_height\n",
    "\n",
    "        html = await page.content()\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        \n",
    "        # Extract all product links\n",
    "        product_links = extract_product_links(soup)\n",
    "        print(f\"Found {len(product_links)} product links.\")\n",
    "        \n",
    "        # Filter out links that are already valid (i.e., already scraped)\n",
    "        new_links = [link for link in product_links if link not in valid_links]\n",
    "        print(f\"Scraping {len(new_links)} new product links.\")\n",
    "        \n",
    "        # Scrape all product details\n",
    "        product_details = await scrape_all_product_details(new_links, page)\n",
    "        \n",
    "        # Clean and format data\n",
    "        cleaned_product_details = clean_and_add_primary_color(product_details, product_type, gender)\n",
    "\n",
    "        # Append the cleaned data to the global list\n",
    "        all_cleaned_product_data.extend(cleaned_product_details)\n",
    "\n",
    "        # Print cleaned product details\n",
    "        for product in cleaned_product_details:\n",
    "            print(product)\n",
    "\n",
    "        await browser.close()\n",
    "\n",
    "def extract_product_links(soup):\n",
    "    base_url = \"https://lamaretail.com\"\n",
    "    links = [base_url + link.get('href') for link in soup.find_all('a', class_='grid-product__link')]\n",
    "    return links\n",
    "\n",
    "async def scrape_all_product_details(product_links, page):\n",
    "    product_details = []\n",
    "    \n",
    "    for link in product_links:\n",
    "        print(f\"Scraping {link}...\")\n",
    "        details = await fetch_product_details(link, page)\n",
    "        if details:\n",
    "            product_details.append(details)\n",
    "    \n",
    "    return product_details\n",
    "\n",
    "async def fetch_product_details(link, page):\n",
    "    attempts = 3\n",
    "    for attempt in range(attempts):\n",
    "        try:\n",
    "            await page.goto(link, timeout=15000)\n",
    "            await page.wait_for_selector('h1.product-single__title', timeout=15000)\n",
    "            await page.wait_for_selector('span.product__price', timeout=15000)\n",
    "            await page.wait_for_selector('div.swatches.swatches-type-products.hover-enabled', timeout=15000)\n",
    "            await page.wait_for_selector('div#swatch-option2', timeout=15000)\n",
    "            await page.wait_for_selector('div.product__thumb-item img', timeout=15000)\n",
    "            \n",
    "            html = await page.content()\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "            # Extract product details\n",
    "            product_name = soup.find('h1', class_='product-single__title').get_text(strip=True) if soup.find('h1', class_='product-single__title') else \"Not Available\"\n",
    "\n",
    "            # Extract price, prioritizing sale price, then regular price, and using fallback if needed\n",
    "            product_price_container = soup.find('div', class_='product-block product-block--price')\n",
    "            product_price = \"Not Available\"\n",
    "\n",
    "            if product_price_container:\n",
    "                sale_price_tag = product_price_container.find('span', class_='product__price on-sale')\n",
    "                regular_price_tag = product_price_container.find('span', class_='product__price product__price--compare')\n",
    "\n",
    "                if sale_price_tag:\n",
    "                    print(\"HERE\")\n",
    "                    sale_price = sale_price_tag.find('span', class_='money').get('doubly-currency-pkr', None)\n",
    "                    if sale_price:\n",
    "                        product_price = f\"{int(sale_price) / 100:.2f}\" if sale_price else \"Not Available\"\n",
    "                    else:\n",
    "                        # Extract the price text\n",
    "                        sale_price = sale_price_tag.find('span', class_='money').get_text(strip=True)\n",
    "                        # Use regex to remove the non-numeric characters and convert to integer\n",
    "                        product_price = re.sub(r'\\D', '', sale_price)\n",
    "                        product_price = f\"{int(product_price) / 100:.2f}\" if product_price else \"Not Available\"\n",
    "\n",
    "                elif regular_price_tag:\n",
    "                    regular_price = regular_price_tag.find('span', class_='money').get('doubly-currency-pkr', None)\n",
    "                    product_price = f\"{int(regular_price) / 100:.2f}\" if regular_price else \"Not Available\"\n",
    "\n",
    "                else:\n",
    "                    product_price_tag = product_price_container.find('span', class_='money')\n",
    "                    raw_price = product_price_tag.get('doubly-currency-pkr', None) if product_price_tag else None\n",
    "                    product_price = f\"{int(raw_price) / 100:.2f}\" if raw_price else \"Not Available\"\n",
    "            else:\n",
    "                product_price_tag = product_price_container.find('span', class_='money')\n",
    "                raw_price = product_price_tag.get('doubly-currency-pkr', None) if product_price_tag else None\n",
    "                product_price = f\"{int(raw_price) / 100:.2f}\" if raw_price else \"Not Available\"\n",
    "            \n",
    "            colors = extract_colors(soup)\n",
    "            sizes = extract_sizes(soup)\n",
    "            \n",
    "            unique_colors = list(dict.fromkeys(colors))\n",
    "            primary_color = max(set(colors), key=colors.count) if colors else \"Not Available\"\n",
    "\n",
    "            # Pass primary color to the download_images function\n",
    "            image_paths = await download_images(soup, product_name, primary_color)\n",
    "\n",
    "            return [product_name, product_price, unique_colors, sizes, primary_color, image_paths, link]\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed for {link} due to error: {e}\")\n",
    "            if attempt == attempts - 1:\n",
    "                print(f\"Skipping {link} after {attempts} failed attempts.\")\n",
    "                return None\n",
    "            await asyncio.sleep(5)\n",
    "\n",
    "def extract_colors(soup):\n",
    "    swatches_container = soup.find('div', class_='swatches swatches-type-products hover-enabled')\n",
    "    return [\n",
    "        item.find('div', class_='swatch-custom-image').get('data-value')\n",
    "        for item in swatches_container.find_all('li', class_='swatch-view-item')\n",
    "        if item.find('div', class_='swatch-custom-image') and item.find('div', class_='swatch-custom-image').get('data-value')\n",
    "    ] if swatches_container else []\n",
    "\n",
    "def extract_sizes(soup):\n",
    "    size_container = soup.find('div', id='swatch-option2')\n",
    "    return [size.get('orig-value') for size in size_container.find_all('li', class_='swatch-view-item')] if size_container else []\n",
    "\n",
    "async def download_images(soup, product_name, primary_color):\n",
    "    image_paths = []\n",
    "    image_container = soup.find_all('div', class_='product__thumb-item')\n",
    "    for idx, img_tag in enumerate(image_container):\n",
    "        img_url = img_tag.find('img')\n",
    "        if img_url and img_url.get('src'):\n",
    "            img_url = 'https:' + img_url['src']\n",
    "            img_data = requests.get(img_url).content\n",
    "            image = Image.open(BytesIO(img_data))\n",
    "\n",
    "            # Save the image to disk with primary color in the filename\n",
    "            image_path = f\"product_images/{product_name}_{primary_color}_{idx + 1}.jpg\"\n",
    "            image.save(image_path)\n",
    "            image_paths.append(image_path)\n",
    "    return image_paths\n",
    "\n",
    "def clean_and_add_primary_color(product_data, product_type, gender):\n",
    "    cleaned_product_data = []\n",
    "    for product in product_data:\n",
    "        if not product:\n",
    "            continue\n",
    "        \n",
    "        product_name, product_price, unique_colors, sizes, primary_color, image_paths, link = product\n",
    "\n",
    "        cleaned_product = {\n",
    "            \"Product\": product_name,\n",
    "            \"Price\": product_price,\n",
    "            \"Colors\": unique_colors,\n",
    "            \"Sizes\": sizes,\n",
    "            \"Primary Color\": primary_color,\n",
    "            \"Link\": link,\n",
    "            \"Images\": image_paths,\n",
    "            \"Type\": product_type,\n",
    "            \"Gender\": gender\n",
    "        }\n",
    "        \n",
    "        cleaned_product_data.append(cleaned_product)\n",
    "    \n",
    "    return cleaned_product_data\n",
    "\n",
    "\n",
    "# Create a folder to save images\n",
    "os.makedirs(\"product_images\", exist_ok=True)\n",
    "\n",
    "# Define a global list to store the cleaned product data\n",
    "all_cleaned_product_data = []\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def scrape_all_categories():\n",
    "    urls = [\n",
    "#         (\"https://lamaretail.com/collections/man-t-shirts\", \"T-Shirt\", \"Men\"),\n",
    "#         (\"https://lamaretail.com/collections/man-hoodies-sweatshirt\", \"Hoodies/Sweatshirts\", \"Men\"),\n",
    "#         (\"https://lamaretail.com/collections/man-sweaters-cardigans\", \"Sweaters/Cardigans\", \"Men\"),\n",
    "#         (\"https://lamaretail.com/collections/man-jackets-coats\", \"Jackets/Coats\", \"Men\"),\n",
    "#         (\"https://lamaretail.com/collections/man-blazers\", \"Blazers\", \"Men\"),\n",
    "#         (\"https://lamaretail.com/collections/man-polo\", \"Polo\", \"Men\"),\n",
    "#         (\"https://lamaretail.com/collections/man-shirts\", \"Shirt\", \"Men\"),\n",
    "#         (\"https://lamaretail.com/collections/man-pants\", \"Bottom\", \"Men\"),\n",
    "#         (\"https://lamaretail.com/collections/man-shorts\", \"Shorts\", \"Men\"),\n",
    "        \n",
    "#         (\"https://lamaretail.com/collections/woman-t-shirts\", \"T-Shirt\", \"Women\"),\n",
    "#         (\"https://lamaretail.com/collections/woman-hoodies-sweatshirt\", \"Hoodies/Sweatshirts\", \"Women\"),\n",
    "#         (\"https://lamaretail.com/collections/fur-fleece\", \"Fur/Fleece\", \"Women\"),\n",
    "#         (\"https://lamaretail.com/collections/woman-blazers\", \"Blazer\", \"Women\"),\n",
    "#         (\"https://lamaretail.com/collections/woman-jeans\", \"Jeans\", \"Women\"),\n",
    "#         (\"https://lamaretail.com/collections/woman-jackets-coats\", \"Jackets/Coats\", \"Women\"),\n",
    "#         (\"https://lamaretail.com/collections/woman-dresses\", \"Dresses/Skirts\", \"Women\"),\n",
    "#         (\"https://lamaretail.com/collections/tops-blouses\", \"Tops/Blouses\", \"Women\"),\n",
    "#         (\"https://lamaretail.com/collections/woman-bodysuits\", \"Bodysuits\", \"Women\"),\n",
    "#         (\"https://lamaretail.com/collections/woman-camisole-bandeaus\", \"Camisole\", \"Women\"),\n",
    "#         (\"https://lamaretail.com/collections/woman-pants\", \"Bottom\", \"Women\"),\n",
    "#         (\"https://lamaretail.com/collections/true-body\", \"TrueBody\", \"Women\"),\n",
    "#         (\"https://lamaretail.com/collections/woman-studio-collection\", \"Studio\", \"Women\"),\n",
    "#         (\"https://lamaretail.com/collections/woman-sweaters-cardigans\", \"Sweaters/Cardigans\", \"Women\"),\n",
    "    ]\n",
    "    \n",
    "    # Create tasks for each category\n",
    "    tasks = [scrape_data(url, product_type, gender) for url, product_type, gender in urls]\n",
    "    await asyncio.gather(*tasks)\n",
    "\n",
    "    \n",
    "await scrape_all_categories()\n",
    "\n",
    "print(len(all_cleaned_product_data))\n",
    "print(all_cleaned_product_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9cbdec7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated product 1: {'Product': 'LILY MID-LENGTH COAT', 'Price': '8970.00', 'Colors': ['KHAKI', 'WHITE', 'BLACK'], 'Sizes': ['SMALL', 'MEDIUM', 'LARGE', 'X-LARGE'], 'Primary Color': 'BLACK', 'Link': 'https://lamaretail.com/collections/woman-jackets-coats/products/lily-mid-length-coat-xlfwcw0003-black', 'Images': ['https://shop-savvy.s3.eu-north-1.amazonaws.com/product_images/LILY MID-LENGTH COAT_BLACK_1.jpg', 'https://shop-savvy.s3.eu-north-1.amazonaws.com/product_images/LILY MID-LENGTH COAT_BLACK_2.jpg', 'https://shop-savvy.s3.eu-north-1.amazonaws.com/product_images/LILY MID-LENGTH COAT_BLACK_3.jpg', 'https://shop-savvy.s3.eu-north-1.amazonaws.com/product_images/LILY MID-LENGTH COAT_BLACK_4.jpg', 'https://shop-savvy.s3.eu-north-1.amazonaws.com/product_images/LILY MID-LENGTH COAT_BLACK_5.jpg', 'https://shop-savvy.s3.eu-north-1.amazonaws.com/product_images/LILY MID-LENGTH COAT_BLACK_6.jpg', 'https://shop-savvy.s3.eu-north-1.amazonaws.com/product_images/LILY MID-LENGTH COAT_BLACK_7.jpg'], 'Type': 'Jackets/Coats', 'Gender': 'Women'}\n",
      "All images uploaded and local paths replaced with S3 URLs.\n"
     ]
    }
   ],
   "source": [
    "# AWS S3 Configuration\n",
    "AWS_ACCESS_KEY = \"AKIAQWHCPYEG5KK2MRGI\"\n",
    "AWS_SECRET_KEY = \"nhxnWuTk3tuzQPi4zrtXB3D/65aNx9VAZXZG104E\"\n",
    "BUCKET_NAME = \"shop-savvy\"\n",
    "REGION = \"eu-north-1\"  # e.g., \"eu-north-1\"\n",
    "\n",
    "# Initialize S3 client\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=AWS_ACCESS_KEY,\n",
    "    aws_secret_access_key=AWS_SECRET_KEY,\n",
    "    region_name=REGION,\n",
    ")\n",
    "\n",
    "# Local image folder\n",
    "local_folder = \"product_images\"\n",
    "os.makedirs(local_folder, exist_ok=True)\n",
    "\n",
    "# Step 1: Optimize and upload images to S3, then update the URLs\n",
    "def optimize_and_upload_image(image_path, product_name, primary_color, idx):\n",
    "    # Open the image\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Resize the image to a reasonable size while maintaining aspect ratio\n",
    "    max_size = (1200, 1200)  # Maximum width and height\n",
    "    image.thumbnail(max_size)\n",
    "\n",
    "    # Compress the image\n",
    "    compressed_image = io.BytesIO()\n",
    "    image.save(compressed_image, format=\"JPEG\", quality=75, optimize=True)\n",
    "    compressed_image.seek(0)\n",
    "\n",
    "    # Step 2: Upload to S3\n",
    "    s3_key = f\"product_images/{product_name}_{primary_color}_{idx + 1}.jpg\"  # Path in the S3 bucket\n",
    "    s3.upload_fileobj(compressed_image, BUCKET_NAME, s3_key)\n",
    "\n",
    "    # Generate the public URL\n",
    "    image_url = f\"https://{BUCKET_NAME}.s3.{REGION}.amazonaws.com/{s3_key}\"\n",
    "    return image_url\n",
    "\n",
    "# Step 3: Process all products and update their 'Images' field with S3 URLs\n",
    "def upload_images_for_all_products(all_cleaned_product_data):\n",
    "    for i, product in enumerate(all_cleaned_product_data):\n",
    "        image_urls = []  # List to store image URLs for the product\n",
    "        \n",
    "        for idx, image_path in enumerate(product['Images']):\n",
    "            # Step 1: For each image path, optimize and upload to S3\n",
    "            image_url = optimize_and_upload_image(image_path, product['Product'], product['Primary Color'], idx)\n",
    "            image_urls.append(image_url)\n",
    "\n",
    "        # Step 2: Replace the local image paths with S3 URLs\n",
    "        all_cleaned_product_data[i]['Images'] = image_urls\n",
    "        \n",
    "        # Print the updated product\n",
    "        print(f\"Updated product {i + 1}: {all_cleaned_product_data[i]}\")\n",
    "\n",
    "# Call the function to process the data\n",
    "upload_images_for_all_products(all_cleaned_product_data)\n",
    "\n",
    "print(\"All images uploaded and local paths replaced with S3 URLs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5f21219d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[{'Product': 'LILY MID-LENGTH COAT', 'Price': '8970.00', 'Colors': ['KHAKI', 'WHITE', 'BLACK'], 'Sizes': ['SMALL', 'MEDIUM', 'LARGE', 'X-LARGE'], 'Primary Color': 'BLACK', 'Link': 'https://lamaretail.com/collections/woman-jackets-coats/products/lily-mid-length-coat-xlfwcw0003-black', 'Images': ['https://shop-savvy.s3.eu-north-1.amazonaws.com/product_images/LILY MID-LENGTH COAT_BLACK_1.jpg', 'https://shop-savvy.s3.eu-north-1.amazonaws.com/product_images/LILY MID-LENGTH COAT_BLACK_2.jpg', 'https://shop-savvy.s3.eu-north-1.amazonaws.com/product_images/LILY MID-LENGTH COAT_BLACK_3.jpg', 'https://shop-savvy.s3.eu-north-1.amazonaws.com/product_images/LILY MID-LENGTH COAT_BLACK_4.jpg', 'https://shop-savvy.s3.eu-north-1.amazonaws.com/product_images/LILY MID-LENGTH COAT_BLACK_5.jpg', 'https://shop-savvy.s3.eu-north-1.amazonaws.com/product_images/LILY MID-LENGTH COAT_BLACK_6.jpg', 'https://shop-savvy.s3.eu-north-1.amazonaws.com/product_images/LILY MID-LENGTH COAT_BLACK_7.jpg'], 'Type': 'Jackets/Coats', 'Gender': 'Women'}]\n"
     ]
    }
   ],
   "source": [
    "print(len(all_cleaned_product_data))\n",
    "print(all_cleaned_product_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b3513fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_copy = pd.DataFrame(all_cleaned_product_data)\n",
    "\n",
    "# Standardize the product names by applying .title() to each product\n",
    "products_copy['Product'] = products_copy['Product'].str.title()\n",
    "\n",
    "# Define the size mapping\n",
    "size_mapping = {\n",
    "    '0X-TRUE': 'XL',\n",
    "    '1X-TRUE': 'XL',\n",
    "    '2X-TRUE': '2XL',\n",
    "    '0X-TRUE (2XL)': '2XL',\n",
    "    '1X-TRUE (3XL)': '3XL',\n",
    "    '2X-TRUE (4XL)': '4XL',\n",
    "    'XXX-LARGE': '3XL',\n",
    "    '3X-TRUE': '3XL',\n",
    "    'SMALL': 'S',\n",
    "    'MEDIUM': 'M',\n",
    "    'LARGE': 'L',\n",
    "    'X-LARGE': 'XL',\n",
    "    'XX-LARGE': '2XL',\n",
    "    'XXL': '2XL',\n",
    "    'X-SMALL': 'XS'\n",
    "}\n",
    "\n",
    "# Apply the size mapping to each row's sizes list\n",
    "products_copy['Sizes'] = products_copy['Sizes'].apply(lambda size_list: [size_mapping.get(size, size) for size in size_list])\n",
    "\n",
    "# Function to update 'Type' based on 'Product'\n",
    "def update_bottom_type(row):\n",
    "    product_name = row['Product'].lower()  # Convert product name to lowercase for case-insensitive comparison\n",
    "    \n",
    "    if 'jean' in product_name or 'jeans' in product_name:\n",
    "        return 'Jeans'\n",
    "    elif 'shorts' in product_name:\n",
    "        return 'Shorts'\n",
    "    else:\n",
    "        return 'Trousers'\n",
    "\n",
    "# Apply the function to rows where Type is 'Bottom'\n",
    "if (products_copy['Type'] == 'Bottom').any():\n",
    "    products_copy.loc[products_copy['Type'] == 'Bottom', 'Type'] = products_copy[products_copy['Type'] == 'Bottom'].apply(update_bottom_type, axis=1)\n",
    "\n",
    "if (products_copy['Type'] == 'Blazer').any():\n",
    "    products_copy.loc[products_copy['Type'] == 'Blazer', 'Type'] = 'Blazers'\n",
    "\n",
    "# Create a dictionary with the mappings\n",
    "type_mappings_copy = {\n",
    "    'Hoodies/Sweatshirts': 'Hoodies & Sweatshirts',\n",
    "    'Sweaters/Cardigans': 'Sweaters & Cardigans',\n",
    "    'Jackets/Coats': 'Jackets & Coats',\n",
    "    'Shirt': 'Shirts',\n",
    "    'Dresses/Skirts': 'Dresses & Skirts',\n",
    "    'Tops/Blouses': 'Tops & Blouses',\n",
    "    'Fur/Fleece': 'Fur & Fleece',\n",
    "    'Camisole': 'Camisole & Bandeaus'\n",
    "}\n",
    "\n",
    "# Apply the mapping to the 'Type' column in 'products_copy'\n",
    "products_copy['Type'] = products_copy['Type'].map(type_mappings_copy).fillna(products_copy['Type'])\n",
    "\n",
    "# Define a function to capitalize the first letter of each word in a color string\n",
    "def format_color(color):\n",
    "    return ' '.join([word.capitalize() for word in color.split()])\n",
    "\n",
    "# Apply this function to the 'Colors' column\n",
    "products_copy['Colors'] = products_copy['Colors'].apply(lambda colors: [format_color(color) for color in colors])\n",
    "\n",
    "products_copy['Primary Color'] = products_copy['Primary Color'].str.title()\n",
    "\n",
    "color_individual_mapping = {\n",
    "    'Black': 'Black',\n",
    "    'All Black': 'Black',\n",
    "    'Charcoal Black': 'Black',\n",
    "    'Black Matte': 'Black',\n",
    "    \n",
    "    'White': 'White',\n",
    "    'Skin': 'White',\n",
    "    'Off White': 'White',\n",
    "    'Ivory': 'White',\n",
    "    'Oat White': 'White',\n",
    "    'Cream': 'White',\n",
    "    'Acru': 'White',\n",
    "    'Ecru': 'White',\n",
    "    'Antique White': 'White',\n",
    "\n",
    "    'Grey': 'Grey',\n",
    "    'Charcoal': 'Grey',\n",
    "    'Metal': 'Grey',\n",
    "    'Dark Grey': 'Grey',\n",
    "    'Anthracite Grey': 'Grey',\n",
    "    'Light Grey': 'Grey',\n",
    "    'Heather Charcoal': 'Grey',\n",
    "    'Melange Grey': 'Grey',\n",
    "    'Heather Grey': 'Grey',\n",
    "    'Slate Grey': 'Grey',\n",
    "    'Dark Grey Marl': 'Grey',\n",
    "    'Medium Grey Marl': 'Grey',\n",
    "    'Light Grey Marl': 'Grey',\n",
    "    'Pale Grey': 'Grey',\n",
    "    'Mid Grey': 'Grey',\n",
    "\n",
    "    'Red': 'Red',\n",
    "    'Dark Red': 'Red',\n",
    "    'Burgundy': 'Red',\n",
    "    'Maroon': 'Red',\n",
    "    'Wine': 'Red',\n",
    "    'Rust': 'Red',\n",
    "    'Brick Red': 'Red',\n",
    "    'Salmon': 'Red',\n",
    "    'Crimson': 'Red',\n",
    "    'Cherry Red': 'Red',\n",
    "    'Deep Maroon': 'Red',\n",
    "\n",
    "    'Blue': 'Blue',\n",
    "    'Dusty Blue': 'Blue',\n",
    "    'Persian Blue': 'Blue',\n",
    "    'Skyway': 'Blue',\n",
    "    'Navy': 'Blue',\n",
    "    'Midnight Blue': 'Blue',\n",
    "    'Crystal Blue': 'Blue',\n",
    "    'Royal Blue': 'Blue',\n",
    "    'Cobalt': 'Blue',\n",
    "    'Cobalt Blue': 'Blue',\n",
    "    'Sky Blue': 'Blue',\n",
    "    'Light Aqua': 'Blue',\n",
    "    'Ice Blue': 'Blue',\n",
    "    'Denim Blue': 'Blue',\n",
    "    'Indigo Blue': 'Blue',\n",
    "    'Mid Blue': 'Blue',\n",
    "    'Light Blue': 'Blue',\n",
    "    'Dark Blue': 'Blue',\n",
    "    'Deep Blue': 'Blue',\n",
    "    'Blue Ice': 'Blue',\n",
    "    'Teal': 'Blue',\n",
    "    'Navy Blue': 'Blue',\n",
    "    'Melange Navy': 'Blue',\n",
    "    'Pale Blue': 'Blue',\n",
    "    'Light Navy': 'Blue',\n",
    "    'Grey Blue': 'Blue',\n",
    "\n",
    "    'Green': 'Green',\n",
    "    'Lime': 'Green',\n",
    "    'Antique Moss': 'Green',\n",
    "    'Dark Green': 'Green',\n",
    "    'Olive': 'Green',\n",
    "    'Dark Olive': 'Green',\n",
    "    'Mid Olive': 'Green',\n",
    "    'Olive Green': 'Green',\n",
    "    'Forest Green': 'Green',\n",
    "    'Mint Green': 'Green',\n",
    "    'Matcha Green': 'Green',\n",
    "    'Peacock': 'Green',\n",
    "    'Emerald': 'Green',\n",
    "    'Grass Green': 'Green',\n",
    "    'Apple Green': 'Green',\n",
    "    'Sea Green': 'Green',\n",
    "    'Light Olive': 'Green',\n",
    "\n",
    "    'Brown': 'Brown',\n",
    "    'Slate Brown': 'Brown',\n",
    "    'Mocha': 'Brown',\n",
    "    'Dark Brown': 'Brown',\n",
    "    'Chocolate': 'Brown',\n",
    "    'Chocolate Brown': 'Brown',\n",
    "    'Coffee': 'Brown',\n",
    "    'Caramel': 'Brown',\n",
    "    'Mushroom': 'Brown',\n",
    "    'Coconut Milk': 'Brown',\n",
    "    'Peanut': 'Brown',\n",
    "    'Honey': 'Brown',\n",
    "    'Tan': 'Brown',\n",
    "    'Camel': 'Brown',\n",
    "    'Spice': 'Brown',\n",
    "    'Mink': 'Brown',\n",
    "    'Taupe': 'Brown',\n",
    "    'Brown Grey': 'Brown',\n",
    "\n",
    "    'Pink': 'Pink',\n",
    "    'Light Pink': 'Pink',\n",
    "    'Vanilla Ice': 'Pink',\n",
    "    'Dirty Pink': 'Pink',\n",
    "    'Blush Pink': 'Pink',\n",
    "    'Tea Pink': 'Pink',\n",
    "    'Baby Coral': 'Pink',\n",
    "    'Blush Coral': 'Pink',\n",
    "    'Pale Pink': 'Pink',\n",
    "    'Pale Coral': 'Pink',\n",
    "    'Blush Coral': 'Pink',\n",
    "    'Dusty Mauve': 'Pink',\n",
    "    'Rusty Pink': 'Pink',\n",
    "\n",
    "    'Purple': 'Purple',\n",
    "    'Plum': 'Purple',\n",
    "    'Plum Purple': 'Purple',\n",
    "    'Lavender Blue': 'Purple',\n",
    "    'Bright Purple': 'Purple',\n",
    "    'Violet': 'Purple',\n",
    "    'Dusty Purple': 'Purple',\n",
    "    'Cloudy Violet': 'Purple',\n",
    "    'Grape': 'Purple',\n",
    "    'Twilight': 'Purple',\n",
    "    'Smoky Grape': 'Purple',\n",
    "\n",
    "    'Yellow': 'Yellow',\n",
    "    'Pale Yellow': 'Yellow',\n",
    "    'Mustard': 'Yellow',\n",
    "    'Golden': 'Yellow',\n",
    "    'Yellow Beige': 'Yellow',\n",
    "    'Stone': 'Yellow',\n",
    "    'Cyberlime': 'Yellow',\n",
    "\n",
    "    'Beige': 'Beige',\n",
    "    'Khaki': 'Beige',\n",
    "    'Khaaki': 'Beige',\n",
    "    'Sand': 'Beige',\n",
    "    'Oatmeal': 'Beige',\n",
    "    'Light Khaki': 'Beige',\n",
    "\n",
    "    'Orange': 'Orange',\n",
    "    'Italian Clay': 'Orange',\n",
    "\n",
    "    'Multi-color': 'Multi-color',\n",
    "    'Multi Color': 'Multi-color',\n",
    "    'Multi Colour': 'Multi-color',\n",
    "    'Multi': 'Multi-color'\n",
    "}\n",
    "\n",
    "# Function to map a single color to its category using the provided color_individual_mapping\n",
    "def map_single_color(color, color_individual_mapping):\n",
    "    # Return the mapped color or 'Other' if the color is not found\n",
    "    return color_individual_mapping.get(color, 'Other')\n",
    "\n",
    "products_copy['FilterColor'] = products_copy['Primary Color'].apply(map_single_color, args=(color_individual_mapping,))\n",
    "# Add a column with 'LAMA' to products_copy\n",
    "products_copy['Brand'] = 'LAMA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d3bace99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where any of the critical columns have \"Not Available\" values\n",
    "products_copy = products_copy[\n",
    "    ~(products_copy['Price'] == 'Not Available') & \n",
    "    ~(products_copy['Primary Color'] == 'Not Available') & \n",
    "    ~(products_copy['Colors'].apply(lambda x: not x))  # Check if the 'Colors' column is empty or 'Not Available'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d8b651ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 1 new records successfully.\n"
     ]
    }
   ],
   "source": [
    "# MongoDB Configuration\n",
    "MONGODB_URI = \"mongodb+srv://AhmadJabbar:0uU29STyRwhoxV0X@shopsavvy.xaqy1.mongodb.net/\"\n",
    "DATABASE_NAME = \"test\"\n",
    "COLLECTION_NAME = \"products\"\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = MongoClient(MONGODB_URI)\n",
    "db = client[DATABASE_NAME]\n",
    "collection = db[COLLECTION_NAME]\n",
    "\n",
    "# Convert DataFrame to MongoDB format\n",
    "records = products_copy.to_dict(orient=\"records\")\n",
    "\n",
    "# Format the records to match MongoDB schema\n",
    "formatted_records = []\n",
    "for record in records:\n",
    "    formatted_record = {\n",
    "        \"product\": record[\"Product\"],\n",
    "        \"price\": int(float(record[\"Price\"])),\n",
    "        \"colors\": record[\"Colors\"],\n",
    "        \"sizes\": record[\"Sizes\"],\n",
    "        \"primary_color\": record[\"Primary Color\"],\n",
    "        \"link\": record[\"Link\"],\n",
    "        \"images\": record[\"Images\"],\n",
    "        \"type\": record[\"Type\"],\n",
    "        \"gender\": record[\"Gender\"],\n",
    "        \"filtercolor\": record[\"FilterColor\"],\n",
    "        \"brand\": record[\"Brand\"],\n",
    "        \"status\": \"valid\"  # Assuming all new entries are valid\n",
    "    }\n",
    "    formatted_records.append(formatted_record)\n",
    "\n",
    "# Insert into MongoDB\n",
    "if formatted_records:\n",
    "    collection.insert_many(formatted_records)\n",
    "    print(f\"Inserted {len(formatted_records)} new records successfully.\")\n",
    "else:\n",
    "    print(\"No records to insert.\")\n",
    "\n",
    "# Close the connection\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6cbcfc57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicates found.\n"
     ]
    }
   ],
   "source": [
    "MONGODB_URI = \"mongodb+srv://AhmadJabbar:0uU29STyRwhoxV0X@shopsavvy.xaqy1.mongodb.net/\"\n",
    "DATABASE_NAME = \"test\"\n",
    "COLLECTION_NAME = \"products\"\n",
    "\n",
    "# MongoDB Utility Functions\n",
    "def get_mongo_client():\n",
    "    return MongoClient(MONGODB_URI)\n",
    "\n",
    "# Function to extract all products and track duplicate IDs based on the 'link' attribute\n",
    "def extract_and_remove_duplicates():\n",
    "    client = get_mongo_client()\n",
    "    db = client[DATABASE_NAME]\n",
    "    collection = db[COLLECTION_NAME]\n",
    "\n",
    "    # Extract all products from the collection\n",
    "    products = list(collection.find())\n",
    "\n",
    "    # Create a list to store duplicate product IDs\n",
    "    duplicate_product_ids = []\n",
    "    seen_links = set()\n",
    "\n",
    "    for product in products:\n",
    "        product_link = product.get('link')\n",
    "        product_id = product.get('_id')\n",
    "\n",
    "        if product_link:\n",
    "            if product_link in seen_links:\n",
    "                # If the link is already seen, it's a duplicate; add the product's ID to the list\n",
    "                duplicate_product_ids.append(product_id)\n",
    "            else:\n",
    "                # If the link is not seen, mark it as seen\n",
    "                seen_links.add(product_link)\n",
    "\n",
    "    # Remove duplicate products based on the collected IDs\n",
    "    if duplicate_product_ids:\n",
    "        collection.delete_many({'_id': {'$in': duplicate_product_ids}})\n",
    "        print(f\"Removed {len(duplicate_product_ids)} duplicate products based on their IDs.\")\n",
    "    else:\n",
    "        print(\"No duplicates found.\")\n",
    "\n",
    "# Call the function to extract duplicates and remove them from MongoDB\n",
    "extract_and_remove_duplicates()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
