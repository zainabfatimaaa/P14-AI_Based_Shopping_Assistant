{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcbb4898",
   "metadata": {},
   "source": [
    "# Outfitters Scraping Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b01c2dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import asyncio\n",
    "import time\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import os\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "from bson import Binary\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import nest_asyncio\n",
    "import re\n",
    "import boto3\n",
    "from slugify import slugify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "35642750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total links fetched: 2092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating links: 100%|█████████████████████| 2092/2092 [02:24<00:00, 14.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid links: 2090\n",
      "Invalid links: 2\n",
      "Removed 2 invalid links from MongoDB.\n",
      "Total valid links: 2090\n"
     ]
    }
   ],
   "source": [
    "# MongoDB Configuration\n",
    "MONGODB_URI = \"mongodb+srv://AhmadJabbar:0uU29STyRwhoxV0X@shopsavvy.xaqy1.mongodb.net/\"\n",
    "DATABASE_NAME = \"test\"\n",
    "COLLECTION_NAME = \"products\"\n",
    "\n",
    "# Create a folder for saving images if it doesn't exist\n",
    "os.makedirs(\"product_images\", exist_ok=True)\n",
    "\n",
    "# MongoDB Utility Functions\n",
    "def get_mongo_client():\n",
    "    return MongoClient(MONGODB_URI)\n",
    "\n",
    "def fetch_all_links():\n",
    "    \"\"\"Fetch all product links from MongoDB.\"\"\"\n",
    "    try:\n",
    "        client = MongoClient(MONGODB_URI)\n",
    "        db = client[DATABASE_NAME]\n",
    "        collection = db[COLLECTION_NAME]\n",
    "        \n",
    "        # fetch only the 'link' field for all products\n",
    "        links = [product['link'] for product in collection.find() if 'link' in product]\n",
    "        print(f\"Total links fetched: {len(links)}\")\n",
    "        return links\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching links: {e}\")\n",
    "        return []\n",
    "    finally:\n",
    "        client.close()\n",
    "\n",
    "def validate_link(link):\n",
    "    \"\"\"Check if the link is valid.\"\"\"\n",
    "    try:\n",
    "        response = requests.head(link, timeout=10)  # Use HEAD request for faster validation\n",
    "        if response.status_code == 200:\n",
    "            return (link, True)  # Link is valid\n",
    "        else:\n",
    "            return (link, False)  # Link is invalid\n",
    "    except requests.RequestException as e:\n",
    "        return (link, False)  # Link is invalid or unreachable\n",
    "\n",
    "def validate_all_links(links):\n",
    "    \"\"\"Validate all links concurrently with progress bar.\"\"\"\n",
    "    valid_links = []\n",
    "    invalid_links = []\n",
    "    \n",
    "    # create a progress bar using tqdm\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor: \n",
    "        futures = {executor.submit(validate_link, link): link for link in links}\n",
    "        \n",
    "        # Use tqdm to show progress for the number of completed tasks\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Validating links\"):\n",
    "            link = futures[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                if result[1]:\n",
    "                    valid_links.append(result[0])\n",
    "                else:\n",
    "                    invalid_links.append(result[0])\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {link}: {e}\")\n",
    "    \n",
    "    print(f\"Valid links: {len(valid_links)}\")\n",
    "    print(f\"Invalid links: {len(invalid_links)}\")\n",
    "    return valid_links, invalid_links\n",
    "\n",
    "def remove_invalid_links_from_mongodb(invalid_links):\n",
    "    \"\"\"Remove invalid links from MongoDB.\"\"\"\n",
    "    try:\n",
    "        client = get_mongo_client()\n",
    "        db = client[DATABASE_NAME]\n",
    "        collection = db[COLLECTION_NAME]\n",
    "        \n",
    "        # Remove the invalid links from the database\n",
    "        collection.delete_many({\"link\": {\"$in\": invalid_links}})\n",
    "        print(f\"Removed {len(invalid_links)} invalid links from MongoDB.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error removing invalid links from MongoDB: {e}\")\n",
    "    finally:\n",
    "        client.close()\n",
    "\n",
    "# Main workflow\n",
    "def main():\n",
    "    # Fetch all links from MongoDB\n",
    "    links = fetch_all_links()\n",
    "    \n",
    "    # Validate all links\n",
    "    valid_links, invalid_links = validate_all_links(links)\n",
    "    \n",
    "    # Remove invalid links from MongoDB\n",
    "    remove_invalid_links_from_mongodb(invalid_links)\n",
    "    \n",
    "    # Optionally, return valid links for further processing\n",
    "    return valid_links\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    valid_links = main()\n",
    "    print(f\"Total valid links: {len(valid_links)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b25a2ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrolling to load all products...\n",
      "Scrolling to load all products...\n",
      "Scrolling to load all products...\n",
      "Scrolling to load all products...\n",
      "Scrolling to load all products...\n",
      "Scrolling to load all products...\n",
      "Found 4 product links.\n",
      "Scraping 0 new product links.\n",
      "Found 1 product links.\n",
      "Scraping 0 new product links.\n",
      "Found 41 product links.\n",
      "Scraping 0 new product links.\n",
      "Found 37 product links.\n",
      "Scraping 0 new product links.\n",
      "Found 57 product links.\n",
      "Scraping 0 new product links.\n",
      "Found 92 product links.\n",
      "Scraping 0 new product links.\n",
      "0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "async def scrape_data(url, product_type, gender):\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(url)\n",
    "\n",
    "        # Scroll to load all products\n",
    "        print(\"Scrolling to load all products...\")\n",
    "        previous_height = await page.evaluate(\"document.body.scrollHeight\")\n",
    "        \n",
    "        while True:\n",
    "            await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "            await page.wait_for_timeout(5000)\n",
    "            \n",
    "            new_height = await page.evaluate(\"document.body.scrollHeight\")\n",
    "            if new_height == previous_height:\n",
    "                break\n",
    "            previous_height = new_height\n",
    "\n",
    "        html = await page.content()\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        \n",
    "        # Extract all product links\n",
    "        product_links = extract_product_links(soup)\n",
    "        print(f\"Found {len(product_links)} product links.\")\n",
    "        \n",
    "        # Filter out links that are already valid (i.e., already scraped)\n",
    "        new_links = [link for link in product_links if link not in valid_links]\n",
    "        print(f\"Scraping {len(new_links)} new product links.\")\n",
    "        \n",
    "        # Scrape all product details\n",
    "        product_details = await scrape_all_product_details(new_links, page)\n",
    "        \n",
    "        # Clean and format data\n",
    "        cleaned_product_details = clean_and_additional_info(product_details, product_type, gender)\n",
    "\n",
    "        # Append the cleaned data to the global list\n",
    "        all_cleaned_product_data.extend(cleaned_product_details)\n",
    "\n",
    "        # Print cleaned product details\n",
    "        for product in cleaned_product_details:\n",
    "            print(product)\n",
    "            print()\n",
    "\n",
    "        await browser.close()\n",
    "\n",
    "\n",
    "def extract_product_links(soup):\n",
    "    base_url = \"https://outfitters.com.pk\"\n",
    "    product_links = []  # Initialize a list to store product links\n",
    "    \n",
    "    # Locate the product grid container\n",
    "    product_grid = soup.find('div', class_='product-grid-container')\n",
    "    \n",
    "    # Traverse the structure to find product links\n",
    "    if product_grid:\n",
    "        for li in product_grid.find_all('li', class_='grid__item grid-item-list'):\n",
    "            card = li.find('div', class_='card card--standard card--media')\n",
    "            if card:\n",
    "                link_tag = card.find('a', href=True)\n",
    "                if link_tag:\n",
    "                    # Combine base URL with the href and add to the list\n",
    "                    product_links.append(base_url.rstrip('/') + '/' + link_tag['href'].lstrip('/'))\n",
    "    \n",
    "    return product_links\n",
    "\n",
    "async def scrape_all_product_details(product_links, page):\n",
    "    product_details = []\n",
    "    \n",
    "    for link in product_links:\n",
    "        print(f\"Scraping {link}...\")\n",
    "        details = await fetch_product_details(link, page)\n",
    "        if details:\n",
    "            product_details.append(details)\n",
    "    \n",
    "    return product_details\n",
    "\n",
    "async def fetch_product_details(link, page):\n",
    "    attempts = 3\n",
    "    for attempt in range(attempts):\n",
    "        try:\n",
    "            await page.goto(link, timeout=15000)\n",
    "            \n",
    "            html = await page.content()\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "            # Extract product name\n",
    "            product_title_div = soup.find('div', class_='product__title')\n",
    "            if product_title_div:\n",
    "                product_name = product_title_div.find('h1').get_text(strip=True) if product_title_div.find('h1') else \"Not Available\"\n",
    "            else:\n",
    "                product_name = \"Not Available\"\n",
    "            \n",
    "            # Extract price from 'price__regular'\n",
    "            price_div = soup.find('div', class_='price__regular')\n",
    "            if price_div:\n",
    "                money_span = price_div.find('span', class_='money')  # Locate the <span> with class 'money'\n",
    "                if money_span:\n",
    "                    price_text = money_span.get_text(strip=True)  # Get the text content\n",
    "                    price_match = re.search(r'\\d+', price_text.replace(\",\", \"\"))  # Extract only the numerical part\n",
    "                    if price_match:\n",
    "                        price = int(price_match.group())\n",
    "                    else:\n",
    "                        price = None\n",
    "                else:\n",
    "                    price = None\n",
    "            else:\n",
    "                price = None\n",
    "            \n",
    "            colors = extract_colors(soup)\n",
    "            primary_color = select_primary_color(colors, soup)\n",
    "            \n",
    "            sizes = extract_sizes(soup)\n",
    "            \n",
    "            # Find the swiper-wrapper div\n",
    "            swiper = soup.find('div', class_='swiper-wrapper')\n",
    "\n",
    "            # Find all img tags inside the swiper-wrapper\n",
    "            img_tags = swiper.find_all('img', {'class': 'zoomImg'})\n",
    "            image_links = []\n",
    "\n",
    "            for img in img_tags:\n",
    "                alt_text = img.get('alt')\n",
    "                if alt_text == primary_color:\n",
    "                    src = img.get('src')\n",
    "                    # Remove the leading \"//\"\n",
    "                    if src.startswith(\"//\"):\n",
    "                        src = src[2:]\n",
    "                    image_links.append(src)\n",
    "\n",
    "            return [product_name, price, colors, primary_color, sizes, image_links, link]\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed for {link} due to error: {e}\")\n",
    "            if attempt == attempts - 1:\n",
    "                print(f\"Skipping {link} after {attempts} failed attempts.\")\n",
    "                return None\n",
    "            await asyncio.sleep(5)\n",
    "\n",
    "def extract_colors(soup):\n",
    "    # Find the color wrapper div\n",
    "    color_wrapper = soup.find('div', class_='color-wrapper')\n",
    "\n",
    "    # Extract the color names\n",
    "    colors = []\n",
    "    if color_wrapper:\n",
    "        # Find all <label> elements within the color wrapper\n",
    "        color_labels = color_wrapper.find_all('label')\n",
    "        for label in color_labels:\n",
    "            # Extract the title attribute (color name)\n",
    "            color_name = label.get('title')\n",
    "            if color_name:\n",
    "                colors.append(color_name)\n",
    "\n",
    "    # Return the list of colors\n",
    "    return colors\n",
    "\n",
    "def select_primary_color(colors, soup):\n",
    "    if not colors:\n",
    "        return \"Not Available\"\n",
    "    \n",
    "    if len(colors) == 1:\n",
    "        return colors[0]  # Return the only color if there's only one\n",
    "    \n",
    "    # Find the script tag with class 'analytics'\n",
    "    script_tag = soup.find('script', class_='analytics')\n",
    "    \n",
    "    if script_tag:\n",
    "        # Get the content of the script tag\n",
    "        script_content = script_tag.string\n",
    "        \n",
    "        # Use regex to find the variant information and extract only the color part (before '/')\n",
    "        match = re.search(r'\"variant\":\"([^\"]+)\"', script_content)\n",
    "        \n",
    "        if match:\n",
    "            variant = match.group(1)\n",
    "            # Extract color part before the first '/'\n",
    "            color_match = re.match(r'([^\\/]+)', variant)  # Match the part before '/'\n",
    "            \n",
    "            if color_match:\n",
    "                color = color_match.group(1)\n",
    "                # Remove backslashes and any unwanted characters\n",
    "                return color.replace('\\\\', '').strip()  # Remove backslashes and extra spaces\n",
    "            else:\n",
    "                return None  # No color part found in the variant string\n",
    "        else:\n",
    "            return None  # No variant found\n",
    "    return None  # No script tag found\n",
    "\n",
    "def extract_sizes(soup):\n",
    "    # Find the div with the class 'size-wrapper'\n",
    "    size_wrapper = soup.find('div', class_='size-wrapper')\n",
    "    \n",
    "    # Initialize an empty list to store sizes\n",
    "    sizes = []\n",
    "    \n",
    "    if size_wrapper:\n",
    "        # Find all input elements of type radio inside the size wrapper\n",
    "        inputs = size_wrapper.find_all('input', type='radio')\n",
    "        \n",
    "        # Loop through all inputs and get the size from the 'value' attribute\n",
    "        for input_tag in inputs:\n",
    "            size_value = input_tag.get('value')\n",
    "            if size_value:\n",
    "                sizes.append(size_value)\n",
    "    \n",
    "    # Return the unique sizes list\n",
    "    return list(set(sizes))\n",
    "\n",
    "\n",
    "async def download_images(soup, product_name, primary_color):\n",
    "    image_paths = []\n",
    "    image_container = soup.find_all('div', class_='product__thumb-item')\n",
    "    for idx, img_tag in enumerate(image_container):\n",
    "        img_url = img_tag.find('img')\n",
    "        if img_url and img_url.get('src'):\n",
    "            img_url = 'https:' + img_url['src']\n",
    "            img_data = requests.get(img_url).content\n",
    "            image = Image.open(BytesIO(img_data))\n",
    "\n",
    "            # Save the image to disk with primary color in the filename\n",
    "            image_path = f\"product_images/{product_name}_{primary_color}_{idx + 1}.jpg\"\n",
    "            image.save(image_path)\n",
    "            image_paths.append(image_path)\n",
    "    return image_paths\n",
    "\n",
    "def clean_and_additional_info(product_data, product_type, gender):\n",
    "    cleaned_product_data = []\n",
    "    for product in product_data:\n",
    "        if not product:\n",
    "            continue\n",
    "        \n",
    "        product_name, product_price, unique_colors, primary_color, sizes, image_links, link = product\n",
    "        \n",
    "        cleaned_product = {\n",
    "            \"Product\": product_name,\n",
    "            \"Price\": product_price,\n",
    "            \"Colors\": unique_colors,\n",
    "            \"Sizes\": sizes,\n",
    "            \"Primary Color\": primary_color,\n",
    "            \"Link\": link,\n",
    "            \"Images\": image_links,\n",
    "            \"Type\": product_type,\n",
    "            \"Gender\": gender\n",
    "        }\n",
    "        \n",
    "        cleaned_product_data.append(cleaned_product)\n",
    "    \n",
    "    return cleaned_product_data\n",
    "\n",
    "# Create a folder to save images\n",
    "os.makedirs(\"product_images2\", exist_ok=True)\n",
    "\n",
    "# Define a global list to store the cleaned product data\n",
    "all_cleaned_product_data = []\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "async def scrape_all_categories():\n",
    "    urls = [\n",
    "#         (\"https://outfitters.com.pk/collections/men-t-shirts\", \"T-Shirt\", \"Men\"),\n",
    "#         (\"https://outfitters.com.pk/collections/men-sweatshirts\", \"Hoodies/Sweatshirts\", \"Men\"),\n",
    "#         (\"https://outfitters.com.pk/collections/men-sweater-1\", \"Sweaters/Cardigans\", \"Men\"),\n",
    "#         (\"https://outfitters.com.pk/collections/men-jacket-1\", \"Jackets/Coats\", \"Men\"),\n",
    "#         (\"https://outfitters.com.pk/collections/men-activewear\", \"Activewear\", \"Men\"),\n",
    "#         (\"https://outfitters.com.pk/collections/men-polo-shirts\", \"Polo\", \"Men\"),\n",
    "#         (\"https://outfitters.com.pk/collections/men-shirts\", \"Shirt\", \"Men\"),\n",
    "        (\"https://outfitters.com.pk/collections/men-denim-collection\", \"Jeans\", \"Men\"),\n",
    "        (\"https://outfitters.com.pk/collections/men-trousers\", \"Trousers\", \"Men\"),\n",
    "        (\"https://outfitters.com.pk/collections/men-shorts\", \"Shorts\", \"Men\"),\n",
    "        (\"https://outfitters.com.pk/collections/women-t-shirts\", \"T-Shirt\", \"Women\"),\n",
    "        (\"https://outfitters.com.pk/collections/women-sweatshirts\", \"Hoodies/Sweatshirts\", \"Women\"),\n",
    "        (\"https://outfitters.com.pk/collections/women-co-ord-sets\", \"CO-ORD\", \"Women\"),\n",
    "#         (\"https://outfitters.com.pk/collections/women-shirts\", \"Shirt\", \"Women\"),\n",
    "#         (\"https://outfitters.com.pk/collections/women-denim-collection\", \"Jeans\", \"Women\"),\n",
    "#         (\"https://outfitters.com.pk/collections/women-jacket-1\", \"Jackets/Coats\", \"Women\"),\n",
    "#         (\"https://outfitters.com.pk/collections/women-dresses-and-jumpsuit\", \"Dresses/Skirts\", \"Women\"),\n",
    "#         (\"https://outfitters.com.pk/collections/women-activewear\", \"Activewear\", \"Women\"),\n",
    "#         (\"https://outfitters.com.pk/collections/women-trouser\", \"Trousers\", \"Women\"),\n",
    "#         (\"https://outfitters.com.pk/collections/women-shorts-sale\", \"Shorts\", \"Women\"),\n",
    "#         (\"https://outfitters.com.pk/collections/women-sweaters-sale\", \"Sweaters/Cardigans\", \"Women\"),\n",
    "        \n",
    "    ]\n",
    "    \n",
    "    # Create tasks for each category\n",
    "    tasks = [scrape_data(url, product_type, gender) for url, product_type, gender in urls]\n",
    "    await asyncio.gather(*tasks)\n",
    "\n",
    "    \n",
    "await scrape_all_categories()\n",
    "\n",
    "print(len(all_cleaned_product_data))\n",
    "print(all_cleaned_product_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9cbdec7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing product 1/1: Basic Sweatshirt\n",
      "Processing image 1/7 for product Basic Sweatshirt\n",
      "Processing image 2/7 for product Basic Sweatshirt\n",
      "Processing image 3/7 for product Basic Sweatshirt\n",
      "Processing image 4/7 for product Basic Sweatshirt\n",
      "Processing image 5/7 for product Basic Sweatshirt\n",
      "Processing image 6/7 for product Basic Sweatshirt\n",
      "Processing image 7/7 for product Basic Sweatshirt\n",
      "Updated product 1/1: Basic Sweatshirt\n"
     ]
    }
   ],
   "source": [
    "# AWS S3 Configuration\n",
    "AWS_ACCESS_KEY = \"AKIAQWHCPYEG5KK2MRGI\"\n",
    "AWS_SECRET_KEY = \"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\"\n",
    "BUCKET_NAME = \"shop-savvy\"\n",
    "REGION = \"eu-north-1\"\n",
    "\n",
    "# Initialize S3 client\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=AWS_ACCESS_KEY,\n",
    "    aws_secret_access_key=AWS_SECRET_KEY,\n",
    "    region_name=REGION,\n",
    ")\n",
    "\n",
    "def optimize_and_upload_image(image_url, product_name, idx):\n",
    "    try:\n",
    "        # Ensure the image URL has the proper scheme (https://)\n",
    "        if not image_url.startswith(\"http\"):\n",
    "            image_url = \"https://\" + image_url\n",
    "\n",
    "        # Download the image from URL\n",
    "        response = requests.get(image_url, stream=True)\n",
    "        response.raise_for_status()  # Raise an error for bad responses (4xx, 5xx)\n",
    "\n",
    "        image = Image.open(io.BytesIO(response.content))\n",
    "\n",
    "        # Resize and optimize the image\n",
    "        max_size = (1200, 1200)\n",
    "        image.thumbnail(max_size)\n",
    "\n",
    "        # Determine format dynamically\n",
    "        image_format = \"JPEG\" if image.format == \"JPEG\" else \"PNG\"\n",
    "\n",
    "        compressed_image = io.BytesIO()\n",
    "        image.save(compressed_image, format=image_format, quality=75, optimize=True)\n",
    "        compressed_image.seek(0)\n",
    "\n",
    "        # Generate a unique product-based S3 key\n",
    "        product_slug = slugify(product_name)  # Converts \"Loose Pleated Trousers\" -> \"loose-pleated-trousers\"\n",
    "        s3_key = f\"{product_slug}/image_{idx}.{image_format.lower()}\"\n",
    "\n",
    "        # Upload the image to S3\n",
    "        s3.upload_fileobj(compressed_image, BUCKET_NAME, s3_key)\n",
    "\n",
    "        # Generate the new public URL for the image\n",
    "        return f\"https://{BUCKET_NAME}.s3.{REGION}.amazonaws.com/{s3_key}\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_url}: {e}\")\n",
    "        return None  # Return None if an image fails\n",
    "\n",
    "# Process each product with an index to track progress\n",
    "for product_idx, product in enumerate(all_cleaned_product_data, 1):  # Start counting from 1 for easier human-readable progress\n",
    "    print(f\"Processing product {product_idx}/{len(all_cleaned_product_data)}: {product['Product']}\")\n",
    "\n",
    "    new_image_urls = []\n",
    "    \n",
    "    for idx, image_url in enumerate(product['Images']):\n",
    "        # Print progress for each image\n",
    "        print(f\"Processing image {idx + 1}/{len(product['Images'])} for product {product['Product']}\")\n",
    "\n",
    "        # Optimize and upload each image\n",
    "        new_image_url = optimize_and_upload_image(image_url, product[\"Product\"], idx)\n",
    "        \n",
    "        if new_image_url:\n",
    "            new_image_urls.append(new_image_url)  # Append only if successful\n",
    "\n",
    "    # Update the product's images list with the new S3 URLs\n",
    "    product['Images'] = new_image_urls\n",
    "\n",
    "    # Print updated product for verification\n",
    "    print(f\"Updated product {product_idx}/{len(all_cleaned_product_data)}: {product['Product']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5f21219d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[{'Product': 'Basic Sweatshirt', 'Price': 1990, 'Colors': ['Black', 'Off White', 'Light Grey Marl'], 'Sizes': ['M', 'L', 'XL', 'S'], 'Primary Color': 'Black', 'Link': 'https://outfitters.com.pk/collections/men-sweatshirts/products/f0484-107?variant=43703865016511', 'Images': ['https://shop-savvy.s3.eu-north-1.amazonaws.com/basic-sweatshirt/image_0.jpeg', 'https://shop-savvy.s3.eu-north-1.amazonaws.com/basic-sweatshirt/image_1.jpeg', 'https://shop-savvy.s3.eu-north-1.amazonaws.com/basic-sweatshirt/image_2.jpeg', 'https://shop-savvy.s3.eu-north-1.amazonaws.com/basic-sweatshirt/image_3.jpeg', 'https://shop-savvy.s3.eu-north-1.amazonaws.com/basic-sweatshirt/image_4.jpeg', 'https://shop-savvy.s3.eu-north-1.amazonaws.com/basic-sweatshirt/image_5.jpeg', 'https://shop-savvy.s3.eu-north-1.amazonaws.com/basic-sweatshirt/image_6.jpeg'], 'Type': 'Hoodies/Sweatshirts', 'Gender': 'Men'}]\n"
     ]
    }
   ],
   "source": [
    "print(len(all_cleaned_product_data))\n",
    "print(all_cleaned_product_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b3513fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_copy = pd.DataFrame(all_cleaned_product_data)\n",
    "\n",
    "# Standardize the product names by applying .title() to each product\n",
    "products_copy['Product'] = products_copy['Product'].str.title()\n",
    "\n",
    "# Define the size mapping\n",
    "size_mapping = {\n",
    "    'XXL': '2XL'\n",
    "}\n",
    "\n",
    "products_copy['Sizes'] = products_copy['Sizes'].apply(lambda size_list: [size_mapping.get(size, size) for size in size_list])\n",
    "\n",
    "# Function to update 'Type' based on 'Product'\n",
    "def update_bottom_type(row):\n",
    "    product_name = row['Product'].lower()  # Convert product name to lowercase for case-insensitive comparison\n",
    "    \n",
    "    if 'jean' in product_name or 'jeans' in product_name:\n",
    "        return 'Jeans'\n",
    "    elif 'shorts' in product_name:\n",
    "        return 'Shorts'\n",
    "    else:\n",
    "        return 'Trousers'\n",
    "\n",
    "# Create a dictionary with the mappings\n",
    "type_mappings = {\n",
    "    'Hoodies/Sweatshirts': 'Hoodies & Sweatshirts',\n",
    "    'Sweaters/Cardigans': 'Sweaters & Cardigans',\n",
    "    'Jackets/Coats': 'Jackets & Coats',\n",
    "    'Shirt': 'Shirts',\n",
    "    'Dresses/Skirts': 'Dresses & Skirts',\n",
    "    'CO-ORD': 'Co-ords'\n",
    "}\n",
    "\n",
    "# Apply the mapping to the 'Type' column in 'products_copy'\n",
    "products_copy['Type'] = products_copy['Type'].map(type_mappings).fillna(products_copy['Type'])\n",
    "\n",
    "color_individual_mapping = {\n",
    "    'Black': 'Black',\n",
    "    'All Black': 'Black',\n",
    "    'Charcoal Black': 'Black',\n",
    "    'Black Matte': 'Black',\n",
    "    \n",
    "    'White': 'White',\n",
    "    'Skin': 'White',\n",
    "    'Off White': 'White',\n",
    "    'Ivory': 'White',\n",
    "    'Oat White': 'White',\n",
    "    'Cream': 'White',\n",
    "    'Acru': 'White',\n",
    "    'Ecru': 'White',\n",
    "    'Antique White': 'White',\n",
    "\n",
    "    'Grey': 'Grey',\n",
    "    'Charcoal': 'Grey',\n",
    "    'Metal': 'Grey',\n",
    "    'Dark Grey': 'Grey',\n",
    "    'Anthracite Grey': 'Grey',\n",
    "    'Light Grey': 'Grey',\n",
    "    'Heather Charcoal': 'Grey',\n",
    "    'Melange Grey': 'Grey',\n",
    "    'Heather Grey': 'Grey',\n",
    "    'Slate Grey': 'Grey',\n",
    "    'Dark Grey Marl': 'Grey',\n",
    "    'Medium Grey Marl': 'Grey',\n",
    "    'Light Grey Marl': 'Grey',\n",
    "    'Pale Grey': 'Grey',\n",
    "    'Mid Grey': 'Grey',\n",
    "\n",
    "    'Red': 'Red',\n",
    "    'Dark Red': 'Red',\n",
    "    'Burgundy': 'Red',\n",
    "    'Maroon': 'Red',\n",
    "    'Wine': 'Red',\n",
    "    'Rust': 'Red',\n",
    "    'Brick Red': 'Red',\n",
    "    'Salmon': 'Red',\n",
    "    'Crimson': 'Red',\n",
    "    'Cherry Red': 'Red',\n",
    "    'Deep Maroon': 'Red',\n",
    "\n",
    "    'Blue': 'Blue',\n",
    "    'Dusty Blue': 'Blue',\n",
    "    'Persian Blue': 'Blue',\n",
    "    'Skyway': 'Blue',\n",
    "    'Navy': 'Blue',\n",
    "    'Midnight Blue': 'Blue',\n",
    "    'Crystal Blue': 'Blue',\n",
    "    'Royal Blue': 'Blue',\n",
    "    'Cobalt': 'Blue',\n",
    "    'Cobalt Blue': 'Blue',\n",
    "    'Sky Blue': 'Blue',\n",
    "    'Light Aqua': 'Blue',\n",
    "    'Ice Blue': 'Blue',\n",
    "    'Denim Blue': 'Blue',\n",
    "    'Indigo Blue': 'Blue',\n",
    "    'Mid Blue': 'Blue',\n",
    "    'Light Blue': 'Blue',\n",
    "    'Dark Blue': 'Blue',\n",
    "    'Deep Blue': 'Blue',\n",
    "    'Blue Ice': 'Blue',\n",
    "    'Teal': 'Blue',\n",
    "    'Navy Blue': 'Blue',\n",
    "    'Melange Navy': 'Blue',\n",
    "    'Pale Blue': 'Blue',\n",
    "    'Light Navy': 'Blue',\n",
    "    'Grey Blue': 'Blue',\n",
    "\n",
    "    'Green': 'Green',\n",
    "    'Lime': 'Green',\n",
    "    'Antique Moss': 'Green',\n",
    "    'Dark Green': 'Green',\n",
    "    'Olive': 'Green',\n",
    "    'Dark Olive': 'Green',\n",
    "    'Mid Olive': 'Green',\n",
    "    'Olive Green': 'Green',\n",
    "    'Forest Green': 'Green',\n",
    "    'Mint Green': 'Green',\n",
    "    'Matcha Green': 'Green',\n",
    "    'Peacock': 'Green',\n",
    "    'Emerald': 'Green',\n",
    "    'Grass Green': 'Green',\n",
    "    'Apple Green': 'Green',\n",
    "    'Sea Green': 'Green',\n",
    "    'Light Olive': 'Green',\n",
    "\n",
    "    'Brown': 'Brown',\n",
    "    'Slate Brown': 'Brown',\n",
    "    'Mocha': 'Brown',\n",
    "    'Dark Brown': 'Brown',\n",
    "    'Chocolate': 'Brown',\n",
    "    'Chocolate Brown': 'Brown',\n",
    "    'Coffee': 'Brown',\n",
    "    'Caramel': 'Brown',\n",
    "    'Mushroom': 'Brown',\n",
    "    'Coconut Milk': 'Brown',\n",
    "    'Peanut': 'Brown',\n",
    "    'Honey': 'Brown',\n",
    "    'Tan': 'Brown',\n",
    "    'Camel': 'Brown',\n",
    "    'Spice': 'Brown',\n",
    "    'Mink': 'Brown',\n",
    "    'Taupe': 'Brown',\n",
    "    'Brown Grey': 'Brown',\n",
    "\n",
    "    'Pink': 'Pink',\n",
    "    'Light Pink': 'Pink',\n",
    "    'Vanilla Ice': 'Pink',\n",
    "    'Dirty Pink': 'Pink',\n",
    "    'Blush Pink': 'Pink',\n",
    "    'Tea Pink': 'Pink',\n",
    "    'Baby Coral': 'Pink',\n",
    "    'Blush Coral': 'Pink',\n",
    "    'Pale Pink': 'Pink',\n",
    "    'Pale Coral': 'Pink',\n",
    "    'Blush Coral': 'Pink',\n",
    "    'Dusty Mauve': 'Pink',\n",
    "    'Rusty Pink': 'Pink',\n",
    "\n",
    "    'Purple': 'Purple',\n",
    "    'Plum': 'Purple',\n",
    "    'Plum Purple': 'Purple',\n",
    "    'Lavender Blue': 'Purple',\n",
    "    'Bright Purple': 'Purple',\n",
    "    'Violet': 'Purple',\n",
    "    'Dusty Purple': 'Purple',\n",
    "    'Cloudy Violet': 'Purple',\n",
    "    'Grape': 'Purple',\n",
    "    'Twilight': 'Purple',\n",
    "    'Smoky Grape': 'Purple',\n",
    "\n",
    "    'Yellow': 'Yellow',\n",
    "    'Pale Yellow': 'Yellow',\n",
    "    'Mustard': 'Yellow',\n",
    "    'Golden': 'Yellow',\n",
    "    'Yellow Beige': 'Yellow',\n",
    "    'Stone': 'Yellow',\n",
    "    'Cyberlime': 'Yellow',\n",
    "\n",
    "    'Beige': 'Beige',\n",
    "    'Khaki': 'Beige',\n",
    "    'Khaaki': 'Beige',\n",
    "    'Sand': 'Beige',\n",
    "    'Oatmeal': 'Beige',\n",
    "    'Light Khaki': 'Beige',\n",
    "\n",
    "    'Orange': 'Orange',\n",
    "    'Italian Clay': 'Orange',\n",
    "\n",
    "    'Multi-color': 'Multi-color',\n",
    "    'Multi Color': 'Multi-color',\n",
    "    'Multi Colour': 'Multi-color',\n",
    "    'Multi': 'Multi-color'\n",
    "}\n",
    "\n",
    "# Function to map a single color to its category using the provided color_individual_mapping\n",
    "def map_single_color(color, color_individual_mapping):\n",
    "    # Return the mapped color or 'Other' if the color is not found\n",
    "    return color_individual_mapping.get(color, 'Other')\n",
    "\n",
    "products_copy['FilterColor'] = products_copy['Primary Color'].apply(map_single_color, args=(color_individual_mapping,))\n",
    "# Add a column with 'Outfitters' to products_copy\n",
    "products_copy['Brand'] = 'Outfitters'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "47af9987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where any of the critical columns have \"Not Available\" values\n",
    "products_copy = products_copy[\n",
    "    ~(products_copy['Price'] == 'Not Available') & \n",
    "    ~(products_copy['Primary Color'] == 'Not Available') & \n",
    "    ~(products_copy['Colors'].apply(lambda x: not x))  # Check if the 'Colors' column is empty or 'Not Available'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d8b651ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 1 new records successfully.\n"
     ]
    }
   ],
   "source": [
    "# MongoDB Configuration\n",
    "MONGODB_URI = \"mongodb+srv://AhmadJabbar:0uU29STyRwhoxV0X@shopsavvy.xaqy1.mongodb.net/\"\n",
    "DATABASE_NAME = \"test\"\n",
    "COLLECTION_NAME = \"products\"\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = MongoClient(MONGODB_URI)\n",
    "db = client[DATABASE_NAME]\n",
    "collection = db[COLLECTION_NAME]\n",
    "\n",
    "# Convert DataFrame to MongoDB format\n",
    "records = products_copy.to_dict(orient=\"records\")\n",
    "\n",
    "# Format the records to match MongoDB schema\n",
    "formatted_records = []\n",
    "for record in records:\n",
    "    formatted_record = {\n",
    "        \"product\": record[\"Product\"],\n",
    "        \"price\": int(float(record[\"Price\"])),\n",
    "        \"colors\": record[\"Colors\"],\n",
    "        \"sizes\": record[\"Sizes\"],\n",
    "        \"primary_color\": record[\"Primary Color\"],\n",
    "        \"link\": record[\"Link\"],\n",
    "        \"images\": record[\"Images\"],\n",
    "        \"type\": record[\"Type\"],\n",
    "        \"gender\": record[\"Gender\"],\n",
    "        \"filtercolor\": record[\"FilterColor\"],\n",
    "        \"brand\": record[\"Brand\"],\n",
    "        \"status\": \"valid\"  # Assuming all new entries are valid\n",
    "    }\n",
    "    formatted_records.append(formatted_record)\n",
    "\n",
    "# Insert into MongoDB\n",
    "if formatted_records:\n",
    "    collection.insert_many(formatted_records)\n",
    "    print(f\"Inserted {len(formatted_records)} new records successfully.\")\n",
    "else:\n",
    "    print(\"No records to insert.\")\n",
    "\n",
    "# Close the connection\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c1e2c236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicates found.\n"
     ]
    }
   ],
   "source": [
    "MONGODB_URI = \"mongodb+srv://AhmadJabbar:0uU29STyRwhoxV0X@shopsavvy.xaqy1.mongodb.net/\"\n",
    "DATABASE_NAME = \"test\"\n",
    "COLLECTION_NAME = \"products\"\n",
    "\n",
    "# MongoDB Utility Functions\n",
    "def get_mongo_client():\n",
    "    return MongoClient(MONGODB_URI)\n",
    "\n",
    "# Function to extract all products and track duplicate IDs based on the 'link' attribute\n",
    "def extract_and_remove_duplicates():\n",
    "    client = get_mongo_client()\n",
    "    db = client[DATABASE_NAME]\n",
    "    collection = db[COLLECTION_NAME]\n",
    "\n",
    "    # Extract all products from the collection\n",
    "    products = list(collection.find())\n",
    "\n",
    "    # Create a list to store duplicate product IDs\n",
    "    duplicate_product_ids = []\n",
    "    seen_links = set()\n",
    "\n",
    "    for product in products:\n",
    "        product_link = product.get('link')\n",
    "        product_id = product.get('_id')\n",
    "\n",
    "        if product_link:\n",
    "            if product_link in seen_links:\n",
    "                # If the link is already seen, it's a duplicate; add the product's ID to the list\n",
    "                duplicate_product_ids.append(product_id)\n",
    "            else:\n",
    "                # If the link is not seen, mark it as seen\n",
    "                seen_links.add(product_link)\n",
    "\n",
    "    # Remove duplicate products based on the collected IDs\n",
    "    if duplicate_product_ids:\n",
    "        collection.delete_many({'_id': {'$in': duplicate_product_ids}})\n",
    "        print(f\"Removed {len(duplicate_product_ids)} duplicate products based on their IDs.\")\n",
    "    else:\n",
    "        print(\"No duplicates found.\")\n",
    "\n",
    "# Call the function to extract duplicates and remove them from MongoDB\n",
    "extract_and_remove_duplicates()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
